{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671cb2de-f219-4ec9-b0f4-db6e6f34c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\rutuj\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\rutuj\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7b393-02a0-4117-8317-e7c9c19ce626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47bfad2-ac94-40d1-b3c5-a79462512859",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_cascade = cv2.CascadeClassifier(r\"C:\\Users\\rutuj\\Downloads\\haarcascade_car.xml\")\n",
    "image = cv2.imread(r\"C:\\Users\\rutuj\\Downloads\\car_parking.png\")\n",
    "if image is None :\n",
    "    print (\"Error : Could not load image.please check the file path\")\n",
    "else :\n",
    "    image = cv2.resize(image,(400,200))\n",
    "    cv2.imshow('original image',image)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    cars = car_cascade.detectMultiScale(gray_image, 1.1, 1)\n",
    "    for (x,y,w,h) in cars :\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "    cv2.imshow('detected cars',image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9fc6a4-c968-415a-87e7-8a55270faae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6e531d-d3a8-4db9-a3cd-1d001082269b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 691 images belonging to 2 classes.\n",
      "Found 691 images belonging to 2 classes.\n",
      "Found 691 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set up directories\n",
    "train_dir = r\"C:\\Users\\rutuj\\Downloads\\forest_fire\\forest_fire\"\n",
    "valid_dir = r\"C:\\Users\\rutuj\\Downloads\\forest_fire\\forest_fire\"\n",
    "test_dir = r\"C:\\Users\\rutuj\\Downloads\\forest_fire\\forest_fire\"\n",
    "\n",
    "# Set up ImageDataGenerators for loading images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load images from directories\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(64, 64), batch_size=32, class_mode='binary')\n",
    "valid_generator = valid_datagen.flow_from_directory(valid_dir, target_size=(64, 64), batch_size=32, class_mode='binary')\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(64, 64), batch_size=32, class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c64dedc-4c3f-47c9-b1c1-8b41ae835f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification: wildfire or no wildfire\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60658d62-b656-4f2b-b092-48e854de01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dab947bf-a23b-4441-b497-867a05c86738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9825 - loss: 0.0510 - val_accuracy: 0.9841 - val_loss: 0.0320\n",
      "Epoch 2/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9826 - loss: 0.0403 - val_accuracy: 0.9841 - val_loss: 0.0274\n",
      "Epoch 3/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.9789 - loss: 0.0349 - val_accuracy: 0.9841 - val_loss: 0.0320\n",
      "Epoch 4/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9811 - loss: 0.0386 - val_accuracy: 0.9841 - val_loss: 0.0322\n",
      "Epoch 5/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.9821 - loss: 0.0350 - val_accuracy: 0.9841 - val_loss: 0.0453\n",
      "Epoch 6/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.9821 - loss: 0.0390 - val_accuracy: 0.9841 - val_loss: 0.0243\n",
      "Epoch 7/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.9871 - loss: 0.0200 - val_accuracy: 0.9841 - val_loss: 0.0193\n",
      "Epoch 8/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.9907 - loss: 0.0161 - val_accuracy: 0.9855 - val_loss: 0.0209\n",
      "Epoch 9/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.9894 - loss: 0.0192 - val_accuracy: 0.9841 - val_loss: 0.0182\n",
      "Epoch 10/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9858 - loss: 0.0196 - val_accuracy: 0.9884 - val_loss: 0.0333\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_generator, validation_data=valid_generator, epochs=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d3a0d-75cc-474c-be41-b7c7ea6b1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"
     ]
    }
   ],
   "source": [
    "# Function to load and predict an image\n",
    "def predict_image():\n",
    "    # Open file dialog to select an image\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        # Display the image in the GUI\n",
    "        img = Image.open(file_path)\n",
    "        img = img.resize((200, 200))\n",
    "        img = ImageTk.PhotoImage(img)\n",
    "        image_label.configure(image=img)\n",
    "        image_label.image = img\n",
    "\n",
    "        # Preprocess the image for the model\n",
    "        img_for_model = Image.open(file_path).resize((64, 64))\n",
    "        img_array = np.array(img_for_model) / 255.0  # Rescale like during training\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Make a prediction\n",
    "        prediction = model.predict(img_array)[0][0]\n",
    "        result = \"Wildfire\" if prediction > 0.5 else \"No Wildfire\"\n",
    "        result_label.config(text=\"Prediction: \" + result)\n",
    "\n",
    "# Setting up the GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Forest Fire Detection\")\n",
    "root.geometry(\"400x400\")\n",
    "\n",
    "# Add widgets\n",
    "btn = tk.Button(root, text=\"Upload Image\", command=predict_image)\n",
    "btn.pack(pady=20)\n",
    "\n",
    "image_label = tk.Label(root)\n",
    "image_label.pack()\n",
    "\n",
    "result_label = tk.Label(root, text=\"Prediction: \", font=(\"Helvetica\", 16))\n",
    "result_label.pack(pady=20)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41746ac-7d20-436f-9b62-7b002bf5a60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a605e62-20a5-4bc1-a9c4-3294477616aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
